\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}\texthl  {Supervised Learning} (lecture 1)}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}\texthl  {Linear Models and Why $\hat  {y} = XW^{T} + b$?}}{1}{}\protected@file@percent }
\newlabel{section:linearconvention}{{1.1}{1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Naive Linear Model\relax }}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}\texthl  {Regression} (lecture 1)}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\texthl  {Linear Regression}}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}\texthl  {Loss Function} (lecture 2)}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\texthl  {Properties of the Loss Function}}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}\texthl  {Gradient Descent} (lecture 2)}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Consequence of choosing $\alpha $, [Cornell CS4780, Lecture 7, Fall 2018]\relax }}{3}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example}{{1}{3}}
\gdef \@abspage@last{3}
